## MIT 6.824 分布式系统笔记

### L3 GFS

#### 分布式的难点

人们设计大型分布式系统或大型存储系统出发点通常是，他们想获取巨大的性能加成，进而利用数百台计算机的资源来同时完成大量工作。因此，性能问题就成为了最初的诉求。 之后，很自然的想法就是将数据分割放到大量的服务器上，这样就可以并行的从多台服务器读取数据。我们将这种方式称之为分片（Sharding）。

如果你在成百上千台服务器进行分片，你将会看见常态的故障。如果你有数千台服务器，那么总是会有一台服务器宕机，每天甚至每个小时都可能会发生错误。所以，我们需要自动化的方法而不是人工介入来修复错误。我们需要一个自动的容错系统，这就引出了容错这个话题（fault tolerance）。

实现容错最有用的一种方法是使用复制，只需要维护2-3个数据的副本，当其中一个故障了，你就可以使用另一个。所以，如果想要容错能力，就得有复制（replication）。

如果有复制，那就有了两份数据的副本。可以确定的是，如果你不小心，它们就会不一致。所以，你本来设想的是，有了两个数据副本，你可以任意使用其中一个副本来容错。但是如果你不够小心，两个数据的副本就不是完全一致，严格来说，它们就不再互为副本了。而你获取到的数据内容也将取决于你向哪个副本请求数据。这对于应用程序来说就有些麻烦了。所以，如果我们有了复制，我们就有不一致的问题（inconsistency）。

通过聪明的设计，你可以避免不一致的问题，并且让数据看起来也表现的符合预期。但是为了达到这样的效果，你总是需要额外的工作，需要不同服务器之间通过网络额外的交互，而这样的交互会降低性能。所以如果你想要一致性，你的代价就是低性能。但这明显不是我们最开始所希望的。

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111174832.png" width="500px"> </div>



#### 一种分布式存储错误的设计

对于强一致性或者好的一致性的设备，从应用程序或者客户端来看就像是在和一台服务器在通信。对于一个理想的强一致性模型，你看到的就像是只有一台服务器，一份数据，并且系统一次只做一件事情。

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111211740.png" width="500px"> </div>

上图表示了一个因服务端接收数据顺序不一样而可能出问题的系统。

#### GFS的设计目标

GFS的目标是构建一个大型的，快速的文件系统。并且这个文件系统是全局有效的，这样各种不同的应用程序都可以从中读取数据。

GFS在各个方面对大型的顺序文件读写进行了定制。GFS只会顺序处理，不支持随机访问。

#### GFS Master节点

接下来是GFS的大致架构。

GFS中只有一个Master节点在工作，Master节点保存了文件名和存储位置的对应关系。除此之外，还有大量的Chunk服务器。Master用来管理文件和Chunk的信息，而Chunk服务器用来存储实际的数据。Master节点知道每一个文件对应的Chunk的ID，这些Chunk是64MB大小，它们共同构成了一个文件。如果我有一个1GB的文件，那么Master节点就知道文件的第一个Chunk存储在哪，第二个Chunk存储在哪，等等。当我想读取这个文件中的任意一个部分时，我需要向Master节点查询对应的Chunk在哪个服务器上，之后我可以直接从Chunk服务器读取对应的Chunk数据。

Master节点内保存的数据内容，主要关心两个表单：

1. 文件名 -> Chunk ID 或者 Chunk Handle数组的对应。这个表单告诉你，文件对应了哪些Chunk

2. Chunk ID -> Chunk 数据 对应关系，包括了：

   - 每个Chunk存储在哪些服务器上，所以这部分是Chunk服务器的列表
- 每个Chunk当前的版本号，所以Master节点必须记住每个Chunk对应的版本号。
   - 所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一。所以，Master节点必须记住哪个Chunk服务器持有主Chunk。
- 并且，主Chunk只能在特定的租约时间内担任主Chunk，所以，Master节点要记住主Chunk的租约过期时间

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111213312.png" width="500px"> </div>

以上数据都在内存中，如果Master故障了，这些数据就都丢失了。为了能让Master重启而不丢失数据，Master节点会同时将数据存储在磁盘上。Master会在磁盘上存储Log，每次有数据变更时，Master会在磁盘的Log中追加一条记录，并生成CheckPoint（备份点）。

1. Chunk Handle的数组（第一个表单）要保存在磁盘上。标记为NV（non-volatile 非易失）这个标记的数据会写到磁盘上

2. Chunk服务器列表不用保存到磁盘上。因为Master节点重启之后可以与所有的Chunk服务器通信，并查询每个Chunk服务器存储了哪些Chunk，标记为V（volatile）

3. 版本号要不要写入磁盘取决于GFS如何工作，我认为需要写入磁盘。之后讨论，先标记为NV

4. 主Chunk的ID，可以确定不用写入磁盘，因为Master节点重启之后会忘记谁是主Chunk，这个时候，Master节点可以安全指定一个新的主Chunk。所以这里标记成V

5. 类似的，租约过期时间也不用写入磁盘，标记成V

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111215459.png" width="500px"> </div>

任何时候，如果文件扩展到达了一个新的64MB，需要新增一个Chunk或者由于指定了新的主Chunk而导致版本号更新了，Master节点需要向磁盘中的Log追加一条记录，我刚刚向这个文件添加了一个新的Chunk或者我刚刚修改了Chunk的版本号。

> 这里在磁盘中维护Log而不是数据库的原因是，数据库本质上来说是某种B树（B-tree）或者Hash table，相比之下，追加Log会非常的高效，因为你可以将最近的多个Log记录一次性的写入磁盘。因为这些数据都是向同一个地址追加，这样只需要等待磁盘的磁碟旋转一次。而对于B树来说，每一份数据都需要在磁盘中随机找个位置写入。所以使用Log可以使得磁盘写入更快一些。

当Master节点故障重启，并重建状态，你不会想要从Log的最开始重建状态，因为Log的最开始可能是很久以前。重启时，Master节点会从Log中的最近一个CheckPoint开始恢复，并逐条执行从CheckPoint开始的Log记录。

#### GFS读文件 Read file

有了之前的基础，接下来会列出GFS读和写的步骤，最后，介绍出现故障之后，系统是如何保持正确的行为。

对于读请求来说，意味着应用或者GFS客户端有一个文件名和它想从文件的某个位置读取的偏移量（offset），应用程序会将这些信息发送给Master节点。Master节点会从自己的File表单中查询文件名，得到Chunk ID的数组。因为每个Chunk都是64MB，所以偏移量除以64MB就可以从数组中得到对应的Chunk ID。之后Master再从Chunk表单中找到存有Chunk的服务器列表，并将列表返回给客户端。所以分两步：

1. 客户端-> 文件名 + 偏移量 -> Master
2. Master-> Chunk handle + 服务器列表 -> 客户端

现在客户端从服务器列表中挑选一个来读取数据。（论文中说客户端会选择一个网络上最近的服务器）客户端可能会连续多次读取同一个Chunk的不同位置。所以，客户端会缓存Chunk和服务器的对应关系，这样不用向Master一次次去请求相同的信息。

接下来，客户端和Chunk通信，将Chunk Handle和偏移量发送给那个Chunk服务器。服务器找到数据，将数据返回给客户端。

#### GFS写文件 Write File 1

GFS写文件的过程更加复杂。我们只讨论客户端的记录追加（Record Append），想把buffer中的数据，追加到这个文件名对应的文件中。对于写文件，客户端会向Master节点发送请求说：我想向这个文件名对应的文件追加数据，请告诉我文件中最后一个Chunk的位置。

当有多个客户端同时写同一个文件时，一个客户端并不能知道文件究竟有多长。这个时候，客户端可以向Master节点查询哪个Chunk服务器保存了文件的最后一个Chunk。对于读文件来说，可以从任何最新的Chunk副本读取数据，但是对于写文件来说，必须要通过Chunk的主副本（Primary Chunk）来写入。对于某个特定的Chunk来说，在某一个时间点，Master不一定指定了Chunk的主副本。所以，写文件的时候，需要考虑Chunk的主副本不存在的情况。

对于Master节点来说，如果发现Chunk的主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器。Master节点的工作就是弄清楚在追加文件时，客户端应该与哪个Chunk服务器进行通信。

每个Chunk可能同时有多个副本，最新的副本是指，副本中保存的版本号与Master中记录的Chunk的版本号一致。Chunk副本中的版本号是由Master节点下发的，所以Master节点知道，对于一个特定的Chunk，哪个版本号是最新的。所以Chunk的版本号在Master节点上需要保存在磁盘这种非易失的存储中。如果版本号在故障中丢失，这时，Master无法区分Chunk服务器的数据是旧的或者是新的。

回到之前讲的，当客户端想要对文件进行追加，但是Master又不知道文件尾的Chunk对应的Primary在哪时，Master会等所有存储了最新Chunk版本的服务器集合完成，然后挑选一个作为Primary，其他的作为Secondary。之后，Master会增加版本号，并将版本号写入磁盘，这样就不会丢失这个版本号数据。接下来，Master节点会向Primary和Secondary副本对应的服务器发送消息并告诉它们，谁是Primary，谁是Secondary，Chunk的新版本是什么。Primary和Secondary服务器都会将版本号存储在本地的磁盘中。这样，当它们因为故障重启时，它们可以向Master报告本地保存的Chunk的实际版本号。

所以，现在我们有了一个Primary，它可以接收来自客户端的请求，并将写请求应用到多个Chunk服务器中。之所以要管理Chunk的版本号，是因为这样Master可以将实际更新Chunk的能力转移给Primary服务器。并且在将版本号更新到Primary和Secondary服务器之后，如果Master节点故障重启，还是可以在相同的Primary和Secondary服务器上继续更新Chunk。

现在，Master节点通知Primary和Secondary服务器，你们可以修改这个Chunk。它还给Primary一个租约，这个租约告诉Primary说，在接下来的60秒中，你将是Primary，60秒之后你必须停止成为Primary。这种机制可以确保我们不会同时有两个Primary，我们之后会再做讨论（之后的问答中有一个专门的问题讨论）。

现在来看GFS论文的图2。假设现在Master节点告诉客户端谁是Primary，谁是Secondary，GFS提出了一种聪明的方法来实现写请求的执行序列。客户端会将要追加的数据发送给Primary和Secondary服务器，这些服务器会将数据写入到一个临时位置。所以最开始，这些数据不会追加到文件中。当所有的服务器都返回确认消息说，已经有了要追加的数据，客户端会向Primary服务器发送一条消息说，你和所有的Secondary服务器都有了要追加的数据，现在我想将这个数据追加到这个文件中。Primary服务器或许会从大量客户端收到大量的并发请求，Primary服务器会以某种顺序，一次只执行一个请求。对于每个客户端的追加数据请求（也就是写请求），Primary会查看当前文件结尾的Chunk，并确保Chunk中有足够的剩余空间，然后将客户端要追加的数据写入Chunk的末尾。并且，Primary会通知所有的Secondary服务器也将客户端要追加的数据写入在它们自己存储的Chunk末尾。这样，包括Primary在内的所有副本，都会收到通知将数据追加在Chunk的末尾。

但是对于Secondary服务器来说，它们可能可以执行成功，也可能会执行失败，比如说磁盘空间不足，比如说故障了，比如说Primary发出的消息网络丢包了。如果Secondary实际真的将数据写入到了本地磁盘存储的Chunk中，它会回复“yes”给Primary。如果所有的Secondary服务器都成功将数据写入，并将“yes”回复给了Primary，并且Primary也收到了这些回复。Primary会向客户端返回写入成功。如果至少一个Secondary服务器没有回复Primary，或者回复了，但是内容却是：抱歉，一些不好的事情发生了，比如说磁盘空间不够，或者磁盘故障了，Primary会向客户端返回写入失败。

GFS论文说，如果客户端从Primary得到写入失败，那么客户端应该重新发起整个追加过程。客户端首先会重新与Master交互，找到文件末尾的Chunk；之后，客户端需要重新发起对于Primary和Secondary的数据追加操作。

#### GFS写文件 Write File 2

这部分主要是写文件操作的问答

> 待续

#### GFS的一致性

这里最重要的部分就是重复我们刚刚讨论过的内容。

当我们追加数据时，面对Chunk的三个副本，当客户端发送了一个追加数据的请求，要将数据A追加到文件末尾，所有的三个副本，包括一个Primary和两个Secondary，都成功的将数据追加到了Chunk，所以Chunk中的第一个记录是A。

![微信截图_20201114161415](C:\Users\jxhnw\Desktop\images\微信截图_20201114161415.png)

假设第二个客户端加入进来，想要追加数据B，但是由于网络问题发送给某个副本的消息丢失了。所以，追加数据B的消息只被两个副本收到，一个是Primary，一个是Secondary。这两个副本都在文件中追加了数据B，所以，现在我们有两个副本有数据B，另一个没有。

![微信截图_20201114161458](C:\Users\jxhnw\Desktop\images\微信截图_20201114161458.png)

之后，第三个客户端想要追加数据C，并且第三个客户端记得下图中左边第一个副本是Primary。Primary选择了偏移量，并将偏移量告诉Secondary，将数据C写在Chunk的这个位置。三个副本都将数据C写在这个位置。

![微信截图_20201114161601](C:\Users\jxhnw\Desktop\images\微信截图_20201114161601.png)

对于数据B来说，客户端会收到写入失败的回复，客户端会重发写入数据B的请求。所以，第二个客户端会再次请求追加数据B，或许这次数据没有在网络中丢包，并且所有的三个副本都成功追加了数据B。现在三个副本都在线，并且都有最新的版本号。

![微信截图_20201114161638](C:\Users\jxhnw\Desktop\images\微信截图_20201114161638.png)

之后，如果一个客户端读文件，则读取到的内容取决于读取的是Chunk的哪个副本。客户端总共可以看到三条数据，但是取决于不同的副本，读取数据的顺序是不一样的。

或许最坏的情况是，一些客户端写文件时，因为其中一个Secondary未能成功执行数据追加操作，客户端从Primary收到写入失败的回复。在客户端重新发送写文件请求之前，客户端就故障了。所以，你有可能进入这种情形：数据D出现在某些副本中，而其他副本则完全没有。

在GFS的这种工作方式下，如果Primary返回写入成功，那么一切都还好，如果Primary返回写入失败，就不是那么好了。Primary返回写入失败会导致不同的副本有完全不同的数据。

>提问：为什么GFS要设计成多个副本不一致？
>
>教授：不明白GFS设计者为什么这么做。GFS可以设计成多个副本完全精确同步的，在lab2和lab3中设计的系统，其中的副本是同步的。你们也会知道，为了保持同步，你们要使用各种各样的技术。

GFS这样设计的理由是足够的简单，但是同时也给应用程序暴露了一些奇怪的数据。这里希望为应用程序提供一个相对简单的写入接口，但应用程序需要容忍读取数据的乱序。如果应用程序不能容忍乱序，应用程序要么可以通过在文件中写入序列号，这样读取的时候能自己识别顺序。

最后，让我花一分钟来介绍GFS在它生涯的前5-10年在Google的出色表现，总的来说，它取得了巨大的成功，许多许多Google的应用都使用了它，许多Google的基础架构，例如BigTable和MapReduce是构建在GFS之上，所以GFS在Google内部广泛被应用。它最严重的局限可能在于，它只有一个Master节点，会带来以下问题：

- Master节点必须为每个文件，每个Chunk维护表单，随着GFS的应用越来越多，这意味着涉及的文件也越来越多，最终Master会耗尽内存来存储文件表单。你可以增加内存，但是单台计算机的内存也是有上限的。所以，这是人们遇到的最早的问题。
- 除此之外，单个Master节点要承载数千个客户端的请求，而Master节点的CPU每秒只能处理数百个请求，尤其Master还需要将部分数据写入磁盘，很快，客户端数量超过了单个Master的能力。
- 另一个问题是，应用程序发现很难处理GFS奇怪的语义（本节最开始介绍的GFS的副本数据的同步，或者可以说不同步）。
- 最后一个问题是，从我们读到的GFS论文中，Master节点的故障切换不是自动的。GFS需要人工干预来处理已经永久故障的Master节点，并更换新的服务器，这可能需要几十分钟甚至更长的而时间来处理。对于某些应用程序来说，这个时间太长了。

### L4 Primary-Backup Replication

#### 复制 Replication

这节主要讲了关于容错（Fault-Tolerance）和复制（Replication）的内容。

容错是为了用来提高可用性。当服务出现故障，例如硬件和网路故障，我们仍然想提供服务，这时候就需要用到复制这个工具。但是，复制也不是万能的，复制能解决什么呢？

复制能解决单台计算机的故障，指单台计算机的fail-stop。例如单纯的停止运行，电源或者网络失效。

但是复制不能处理软件中的bug和硬件中的缺陷。如果软件或者硬件有bug，那么复制对我们没有任何帮助。

当然，足够幸运的话，复制也可以处理一些硬件和软件的bug。总的来说，我们还是只能期望复制能处理fail-stop错误。

另一个关于复制的问题：复制所需的资源消耗是否值得？

#### 状态转移和复制状态机

复制有两种方法：**状态转移**（State Transfer）和**复制状态机**（Replicated State Machine），这门课中我们主要介绍后者。

如果有一个服务器的两个副本，让其保持同步，这样Primary出现故障的时候，Backup有所有的信息，就可以接管服务。状态转移的思想：Primary将自己完整的状态（比如内存中的内容），拷贝并发送给Backup。当Primary故障了，Backup就从保存的最新的状态开始运行。VMware FT没有采用这种方法，因为需要通过网络发送的数据量太多。为了提升效率，只发送上次同步后变更的内容就行了。

复制状态机基于：我们想复制的大部分的服务或者计算机软件都有一些确定的内部操作，不确定的部分是外部的输入。通常情况下，如果一台计算机没有外部影响，它只是一个接一个的执行指令，每条指令执行的是计算机中内存和寄存器上确定的函数，只有当外部事件干预时，才会发生一些预期外的事。

所以，复制状态机不会在不同的副本之间发送状态，相应的，它只会从Primary将这些外部事件发送给Backup。通常来说，如果有两台计算机，如果它们从相同的状态开始，并且它们以相同的顺序，在相同的时间，看到了相同的输入，那么它们会一直互为副本，并且一直保持一致。

状态转移传输的是可能是内存，而复制状态机会将来自客户端的操作或者其他外部事件，从Primary传输到Backup。



