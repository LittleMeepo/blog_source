## MIT 6.824 分布式系统笔记

### L3 GFS

#### 分布式的难点

人们设计大型分布式系统或大型存储系统出发点通常是，他们想获取巨大的性能加成，进而利用数百台计算机的资源来同时完成大量工作。因此，性能问题就成为了最初的诉求。 之后，很自然的想法就是将数据分割放到大量的服务器上，这样就可以并行的从多台服务器读取数据。我们将这种方式称之为分片（Sharding）。

如果你在成百上千台服务器进行分片，你将会看见常态的故障。如果你有数千台服务器，那么总是会有一台服务器宕机，每天甚至每个小时都可能会发生错误。所以，我们需要自动化的方法而不是人工介入来修复错误。我们需要一个自动的容错系统，这就引出了容错这个话题（fault tolerance）。

实现容错最有用的一种方法是使用复制，只需要维护2-3个数据的副本，当其中一个故障了，你就可以使用另一个。所以，如果想要容错能力，就得有复制（replication）。

如果有复制，那就有了两份数据的副本。可以确定的是，如果你不小心，它们就会不一致。所以，你本来设想的是，有了两个数据副本，你可以任意使用其中一个副本来容错。但是如果你不够小心，两个数据的副本就不是完全一致，严格来说，它们就不再互为副本了。而你获取到的数据内容也将取决于你向哪个副本请求数据。这对于应用程序来说就有些麻烦了。所以，如果我们有了复制，我们就有不一致的问题（inconsistency）。

通过聪明的设计，你可以避免不一致的问题，并且让数据看起来也表现的符合预期。但是为了达到这样的效果，你总是需要额外的工作，需要不同服务器之间通过网络额外的交互，而这样的交互会降低性能。所以如果你想要一致性，你的代价就是低性能。但这明显不是我们最开始所希望的。

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111174832.png" width="500px"> </div>



#### 一种分布式存储错误的设计

对于强一致性或者好的一致性的设备，从应用程序或者客户端来看就像是在和一台服务器在通信。对于一个理想的强一致性模型，你看到的就像是只有一台服务器，一份数据，并且系统一次只做一件事情。

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111211740.png" width="500px"> </div>

上图表示了一个因服务端接收数据顺序不一样而可能出问题的系统。

#### GFS的设计目标

GFS的目标是构建一个大型的，快速的文件系统。并且这个文件系统是全局有效的，这样各种不同的应用程序都可以从中读取数据。

GFS在各个方面对大型的顺序文件读写进行了定制。GFS只会顺序处理，不支持随机访问。

#### GFS Master节点

接下来是GFS的大致架构。

GFS中只有一个Master节点在工作，Master节点保存了文件名和存储位置的对应关系。除此之外，还有大量的Chunk服务器。Master用来管理文件和Chunk的信息，而Chunk服务器用来存储实际的数据。Master节点知道每一个文件对应的Chunk的ID，这些Chunk是64MB大小，它们共同构成了一个文件。如果我有一个1GB的文件，那么Master节点就知道文件的第一个Chunk存储在哪，第二个Chunk存储在哪，等等。当我想读取这个文件中的任意一个部分时，我需要向Master节点查询对应的Chunk在哪个服务器上，之后我可以直接从Chunk服务器读取对应的Chunk数据。

Master节点内保存的数据内容，主要关心两个表单：

1. 文件名 -> Chunk ID 或者 Chunk Handle数组的对应。这个表单告诉你，文件对应了哪些Chunk

2. Chunk ID -> Chunk 数据 对应关系，包括了：

   - 每个Chunk存储在哪些服务器上，所以这部分是Chunk服务器的列表
- 每个Chunk当前的版本号，所以Master节点必须记住每个Chunk对应的版本号。
   - 所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一。所以，Master节点必须记住哪个Chunk服务器持有主Chunk。
- 并且，主Chunk只能在特定的租约时间内担任主Chunk，所以，Master节点要记住主Chunk的租约过期时间

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111213312.png" width="500px"> </div>

以上数据都在内存中，如果Master故障了，这些数据就都丢失了。为了能让Master重启而不丢失数据，Master节点会同时将数据存储在磁盘上。Master会在磁盘上存储Log，每次有数据变更时，Master会在磁盘的Log中追加一条记录，并生成CheckPoint（备份点）。

1. Chunk Handle的数组（第一个表单）要保存在磁盘上。标记为NV（non-volatile 非易失）这个标记的数据会写到磁盘上

2. Chunk服务器列表不用保存到磁盘上。因为Master节点重启之后可以与所有的Chunk服务器通信，并查询每个Chunk服务器存储了哪些Chunk，标记为V（volatile）

3. 版本号要不要写入磁盘取决于GFS如何工作，我认为需要写入磁盘。之后讨论，先标记为NV

4. 主Chunk的ID，可以确定不用写入磁盘，因为Master节点重启之后会忘记谁是主Chunk，这个时候，Master节点可以安全指定一个新的主Chunk。所以这里标记成V

5. 类似的，租约过期时间也不用写入磁盘，标记成V

<div align='center'> <img src="https://cdn.jsdelivr.net/gh/LittleMeepo/blog_images/images/6.824/20201111215459.png" width="500px"> </div>

任何时候，如果文件扩展到达了一个新的64MB，需要新增一个Chunk或者由于指定了新的主Chunk而导致版本号更新了，Master节点需要向磁盘中的Log追加一条记录，我刚刚向这个文件添加了一个新的Chunk或者我刚刚修改了Chunk的版本号。

> 这里在磁盘中维护Log而不是数据库的原因是，数据库本质上来说是某种B树（B-tree）或者Hash table，相比之下，追加Log会非常的高效，因为你可以将最近的多个Log记录一次性的写入磁盘。因为这些数据都是向同一个地址追加，这样只需要等待磁盘的磁碟旋转一次。而对于B树来说，每一份数据都需要在磁盘中随机找个位置写入。所以使用Log可以使得磁盘写入更快一些。

当Master节点故障重启，并重建状态，你不会想要从Log的最开始重建状态，因为Log的最开始可能是很久以前。重启时，Master节点会从Log中的最近一个CheckPoint开始恢复，并逐条执行从CheckPoint开始的Log记录。

#### GFS读文件 Read file

有了之前的基础，接下来会列出GFS读和写的步骤，最后，介绍出现故障之后，系统是如何保持正确的行为。

对于读请求来说，意味着应用或者GFS客户端有一个文件名和它想从文件的某个位置读取的偏移量（offset），应用程序会将这些信息发送给Master节点。Master节点会从自己的File表单中查询文件名，得到Chunk ID的数组。因为每个Chunk都是64MB，所以偏移量除以64MB就可以从数组中得到对应的Chunk ID。之后Master再从Chunk表单中找到存有Chunk的服务器列表，并将列表返回给客户端。所以分两步：

1. 客户端-> 文件名 + 偏移量 -> Master
2. Master-> Chunk handle + 服务器列表 -> 客户端

现在客户端从服务器列表中挑选一个来读取数据。

### L4 Primary-Backup Replication

#### 复制 Replication

这节主要讲了关于容错（Fault-Tolerance）和复制（Replication）的内容。

容错是为了用来提高可用性。当服务出现故障，例如硬件和网路故障，我们仍然想提供服务，这时候就需要用到复制这个工具。但是，复制也不是万能的，复制能解决什么呢？

复制能解决单台计算机的故障，指单台计算机的fail-stop。例如单纯的停止运行，电源或者网络失效。

但是复制不能处理软件中的bug和硬件中的缺陷。如果软件或者硬件有bug，那么复制对我们没有任何帮助。

当然，足够幸运的话，复制也可以处理一些硬件和软件的bug。总的来说，我们还是只能期望复制能处理fail-stop错误。

另一个关于复制的问题：复制所需的资源消耗是否值得？

#### 状态转移和复制状态机

复制有两种方法：**状态转移**（State Transfer）和**复制状态机**（Replicated State Machine），这门课中我们主要介绍后者。

如果有一个服务器的两个副本，让其保持同步，这样Primary出现故障的时候，Backup有所有的信息，就可以接管服务。状态转移的思想：Primary将自己完整的状态（比如内存中的内容），拷贝并发送给Backup。当Primary故障了，Backup就从保存的最新的状态开始运行。VMware FT没有采用这种方法，因为需要通过网络发送的数据量太多。为了提升效率，只发送上次同步后变更的内容就行了。

复制状态机基于：我们想复制的大部分的服务或者计算机软件都有一些确定的内部操作，不确定的部分是外部的输入。通常情况下，如果一台计算机没有外部影响，它只是一个接一个的执行指令，每条指令执行的是计算机中内存和寄存器上确定的函数，只有当外部事件干预时，才会发生一些预期外的事。

所以，复制状态机不会在不同的副本之间发送状态，相应的，它只会从Primary将这些外部事件发送给Backup。通常来说，如果有两台计算机，如果它们从相同的状态开始，并且它们以相同的顺序，在相同的时间，看到了相同的输入，那么它们会一直互为副本，并且一直保持一致。

状态转移传输的是可能是内存，而复制状态机会将来自客户端的操作或者其他外部事件，从Primary传输到Backup。



